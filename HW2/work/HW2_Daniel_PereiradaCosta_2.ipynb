{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 15:12:56 WARN Utils: Your hostname, Daniels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.26.160.85 instead (on interface en0)\n",
      "23/02/20 15:12:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/20 15:12:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../resource/asnlib/publicdata/small1.csv'\n",
    "# filepath = '../resource/asnlib/publicdata/ta_feng_all_months_merged.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read File, skipping header\n",
    "review = sc.textFile(filepath).zipWithIndex().filter(lambda x: x[1] > 0).map(lambda line: line[0].split(\",\"))\n",
    "if case_number == 1:\n",
    "    review = review.map(lambda x: (x[0], x[1]))\n",
    "elif case_number == 2:\n",
    "    review = review.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "else:\n",
    "    print(\"Invalid case number\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_rdd = review.groupByKey().mapValues(set).map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_basket_memory = basket_rdd.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A-priori algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ['102', '100', '101', '98']),\n",
       " ('4', ['102', '97', '101', '99', '103']),\n",
       " ('8', ['102', '97', '104', '99', '103', '98']),\n",
       " ('9', ['99', '97', '98']),\n",
       " ('10', ['97', '98'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_basket_memory[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_items(basket_rdd: list) -> dict:\n",
    "    '''Translates item names into integers from 1 to n\n",
    "\n",
    "    Args:\n",
    "        basket_rdd (list): [(key, value), ...]\n",
    "\n",
    "    Returns:\n",
    "        (dict): {item: hash, ...}\n",
    "    '''\n",
    "    subset_items = set()\n",
    "    for key, list_items in basket_rdd:\n",
    "        subset_items = subset_items | set(list_items)\n",
    "    subset_items = list(subset_items)\n",
    "    n = range(1, len(subset_items) + 1) # n is the number of distinct items\n",
    "    map_to_int = { item: n_item for item, n_item in zip(subset_items, n)}\n",
    "    return map_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_basket_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_item_hash = hash_items(user_basket_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_basket_memory_hashed = []\n",
    "for key, list_items in user_basket_memory:\n",
    "    user_basket_memory_hashedlist = [map_item_hash[item] for item in list_items]\n",
    "    user_basket_memory_key = key\n",
    "    user_basket_memory_hashed.append((user_basket_memory_key, user_basket_memory_hashedlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangular_matrix_method():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_item_hash.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(itertools.combinations(map_item_hash.values(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_candidate = (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = set(itertools.combinations(new_candidate, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_1 = [(1,), (3,), (4,), (2,)]\n",
    "# b,c,j,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# { {b,m} {b,c}  {c,m}  {c,j} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_k_1 = [(1,2), (1,3), (3,2), (3,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (3, 2), (3, 4)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_k_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([(1,2)]) <= set(L_k_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([(2,1)]).intersection(set(L_k_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (3, 2), (3, 4)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_k_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,), (3,), (4,), (2,)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = new_candidate(subset, L_k_1, L_1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = (1,)\n",
    "L_1 = [(1,), (3,), (4,), (2,)]\n",
    "L_k_1 = [(1,), (3,), (4,), (2,)]\n",
    "# b,c,j,m\n",
    "#b,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = new_candidate(subset, L_k_1, L_1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m L_k_1\u001b[39m.\u001b[39madd({(\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m), {\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m}})\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'set'"
     ]
    }
   ],
   "source": [
    "a.add({(2,1), {2,2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_k_1.update([(3,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1,), (2,), (2, 3), (3,), (3, 2), (3, 3), (5,), (7,), (8,), (11,), (12,)}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_k_1 | {(3,3), (3,2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-priori Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_candidate(subset: tuple, L_k_1: set, L_1: set, k: int, C_k: list) -> set:\n",
    "    list_of_candidates = set()\n",
    "    for frequent_item in L_1: # Iterate over each item of L_1 that does not intersect\n",
    "        if frequent_item[0] in subset: # Do nothing if L_1 item is already in subset\n",
    "            continue\n",
    "        # For each basket, we need only look at those items that are in L_1\n",
    "        new_candidate = set(sorted(subset + frequent_item)) \n",
    "        \n",
    "        # print(new_candidate)\n",
    "        # print(list_of_candidates)\n",
    "        if new_candidate not in list_of_candidates:\n",
    "            # Examine each pair and determine whether or not that pair is in L_k - 1\n",
    "            subset_combination = set(itertools.combinations(new_candidate, k-1))\n",
    "            # print(subset_combination)\n",
    "            # print(subset_combination)\n",
    "            if subset_combination <= L_k_1:\n",
    "                list_of_candidates.add(tuple(new_candidate)) #if new_candidate not in C_k else list_of_candidates # Remove duplicates\n",
    "    return list_of_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(L_k_1: set, L_1: set, k: int) -> set:\n",
    "    # Generatin C_k from L_k_1 and L_1\n",
    "    C_k = set()\n",
    "    for subset in L_k_1:\n",
    "        new_subsets_candidates = new_candidate(subset, L_k_1, L_1, k, C_k)\n",
    "        for new_subset in new_subsets_candidates:\n",
    "            C_k.add(new_subset) #if new_subsets_candidates not in C_k else C_k\n",
    "            # break\n",
    "\n",
    "    return C_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_itemsets_of_size_k(basket_rdd: list, candidates_list: list, threshold: int) -> set:\n",
    "    '''\n",
    "    Count k-size candidate itemsets and returns those that are frequent\n",
    "\n",
    "    Args:\n",
    "        basket_rdd (list)\n",
    "        candidates_list (list)\n",
    "        threshold (int)\n",
    "    Returns:\n",
    "        \n",
    "    '''\n",
    "    candidates_count = defaultdict(int)\n",
    "    for _, item_list in basket_rdd:\n",
    "        for candidate in candidates_list:\n",
    "            if set(candidate) <= set(item_list):\n",
    "                candidates_count[candidate] += 1 # increase 1 count\n",
    "\n",
    "    return {k for k,v in candidates_count.items() if v >= threshold}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hash_items(basket_rdd: list) -> dict:\n",
    "#     '''Translates item names into integers from 1 to n\n",
    "\n",
    "#     Args:\n",
    "#         basket_rdd (list): [(key, value), ...]\n",
    "\n",
    "#     Returns:\n",
    "#         (dict): {item: hash, ...}\n",
    "#     '''\n",
    "#     subset_items = set()\n",
    "#     for key, list_items in basket_rdd:\n",
    "#         subset_items = subset_items | set(list_items)\n",
    "#     subset_items = list(subset_items)\n",
    "#     n = range(1, len(subset_items) + 1) # n is the number of distinct items\n",
    "#     map_to_int = { item: n_item for item, n_item in zip(subset_items, n)}\n",
    "#     return map_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_items(basket_rdd: list) -> dict:\n",
    "    '''Translates item names into integers from 1 to n\n",
    "\n",
    "    Args:\n",
    "        basket_rdd (list): [(key, value), ...]\n",
    "\n",
    "    Returns:\n",
    "        (dict): {item: hash, ...}\n",
    "    '''\n",
    "    subset_items = set()\n",
    "    for _, list_items in basket_rdd:\n",
    "        subset_items = subset_items | set(list_items)\n",
    "    subset_items = list(subset_items)\n",
    "    n = range(1, len(subset_items) + 1) # n is the number of distinct items\n",
    "    map_to_int = { item: n_item for item, n_item in zip(subset_items, n)}\n",
    "    return map_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at collect at /var/folders/68/6vxh_8k15_n5m7tsxrjkzsvm0000gn/T/ipykernel_43292/1166133831.py:1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basket_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_basket_memory_hashed = []\n",
    "# for key, list_items in basket_rdd.collect():\n",
    "#     user_basket_memory_hashedlist = [map_item_hash[item] for item in list_items]\n",
    "#     user_basket_memory_key = key\n",
    "#     user_basket_memory_hashed.append((user_basket_memory_key, user_basket_memory_hashedlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('103',)\n",
      "('100',)\n",
      "('99',)\n",
      "('97',)\n",
      "('98',)\n",
      "('101',)\n",
      "('105',)\n",
      "('102',)\n"
     ]
    }
   ],
   "source": [
    "chunk_p_s_threshold = 3\n",
    "# Hash partition data since items can be strings (higher performance)\n",
    "partitionData = basket_rdd.collect()\n",
    "map_item_hash = hash_items(partitionData)\n",
    "inv_map_item_hash = {v: k for k, v in map_item_hash.items()} # Look-up table to convert back to original value\n",
    "user_basket_memory_hashed = []\n",
    "for key, list_items in partitionData:\n",
    "    user_basket_memory_hashedlist = [map_item_hash[item] for item in list_items]\n",
    "    user_basket_memory_key = key\n",
    "    user_basket_memory_hashed.append((user_basket_memory_key, user_basket_memory_hashedlist))\n",
    "\n",
    "# A-Pripri Algorithm \n",
    "k = 1\n",
    "C_k = [(item,) for item in map_item_hash.values()] # Singletons Candidates\n",
    "L_k_1 = count_itemsets_of_size_k(user_basket_memory_hashed, C_k, chunk_p_s_threshold) # all frequent singletons\n",
    "L_1 = L_k_1.copy()\n",
    "k += 1\n",
    "while len(L_k_1) > 0:\n",
    "    for freq_item in L_k_1:\n",
    "        print(tuple([inv_map_item_hash[item] for item in freq_item])) # Convert back to \n",
    "    C_k = generate_candidates(L_k_1, L_1, k)\n",
    "    L_k_1 = count_itemsets_of_size_k(user_basket_memory_hashed, C_k, chunk_p_s_threshold) # all frequent singletons\n",
    "    k += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "(3, 7)\n",
      "(4, 12)\n",
      "(5, 7)\n",
      "(3, 8)\n",
      "(5, 11)\n",
      "(8, 12)\n",
      "(3, 10)\n",
      "(4, 5)\n",
      "(3, 12)\n",
      "(4, 11)\n",
      "(8, 11)\n",
      "(7, 11)\n",
      "(4, 7)\n",
      "(3, 5)\n",
      "(4, 8)\n",
      "(7, 8)\n",
      "(4, 10)\n",
      "(3, 11)\n",
      "(11, 12)\n"
     ]
    }
   ],
   "source": [
    "for i in L_k_1:\n",
    "    print(tuple(sorted(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_s_threshold(partitionData: map, support_threshold: int, size_input_file: int) -> int:\n",
    "    '''\n",
    "    Lower the support threshold from s to ps if each Map task gets fraction p of the total input file\n",
    "    \n",
    "    Args:\n",
    "        partitionData (map)\n",
    "        support_threshold (int): s\n",
    "        size_input_file (int)\n",
    "    Returns:\n",
    "        (int): p*s\n",
    "    '''\n",
    "\n",
    "    return round(support_threshold*(len(partitionData)/size_input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_priori_algorithm(partitionData: map, threshold: int, size_input_file: int) -> tuple:\n",
    "\n",
    "    partitionData = list(partitionData)\n",
    "    # Compute Threshold\n",
    "    chunk_p_s_threshold = p_s_threshold(partitionData, threshold, size_input_file)\n",
    "\n",
    "    # Hash partition data since items can be strings (higher performance)\n",
    "    map_item_hash = hash_items(partitionData)\n",
    "    inv_map_item_hash = {v: k for k, v in map_item_hash.items()} # Look-up table to convert back to original value\n",
    "    user_basket_memory_hashed = []\n",
    "    for key, list_items in partitionData:\n",
    "        user_basket_memory_hashedlist = [map_item_hash[item] for item in list_items]\n",
    "        user_basket_memory_key = key\n",
    "        user_basket_memory_hashed.append((user_basket_memory_key, user_basket_memory_hashedlist))\n",
    "\n",
    "    # A-Pripri Algorithm \n",
    "    k = 1\n",
    "    C_k = [(item,) for item in map_item_hash.values()] # Singletons Candidates\n",
    "    L_k_1 = count_itemsets_of_size_k(user_basket_memory_hashed, C_k, chunk_p_s_threshold) # all frequent singletons\n",
    "    L_1 = L_k_1.copy()\n",
    "    k += 1\n",
    "    while len(L_k_1) > 0:\n",
    "        for freq_item in L_k_1:  yield (tuple([inv_map_item_hash[item] for item in tuple(sorted(freq_item))]), 1) # Convert back to \n",
    "        C_k = generate_candidates(L_k_1, L_1, k)\n",
    "        L_k_1 = count_itemsets_of_size_k(user_basket_memory_hashed, C_k, chunk_p_s_threshold) # all frequent singletons\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply to rdd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_threshold = 4\n",
    "size_input_file = basket_rdd.count()\n",
    "map_task_1 = basket_rdd.\\\n",
    "    mapPartitions(lambda partition: a_priori_algorithm(partition, support_threshold, size_input_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_task_1 = map_task_1.map(lambda x: tuple(sorted(x[0]))).distinct().sortBy(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('103',),\n",
       " ('101',),\n",
       " ('97',),\n",
       " ('99',),\n",
       " ('105',),\n",
       " ('98',),\n",
       " ('102',),\n",
       " ('100',),\n",
       " ('102', '98'),\n",
       " ('101', '99'),\n",
       " ('103', '97'),\n",
       " ('100', '102'),\n",
       " ('103', '99'),\n",
       " ('100', '98'),\n",
       " ('97', '99'),\n",
       " ('103', '105'),\n",
       " ('105', '99'),\n",
       " ('101', '97'),\n",
       " ('101', '102'),\n",
       " ('102', '103'),\n",
       " ('98', '99'),\n",
       " ('100', '101'),\n",
       " ('97', '98'),\n",
       " ('102', '97'),\n",
       " ('101', '98'),\n",
       " ('102', '99'),\n",
       " ('103', '98'),\n",
       " ('105', '98'),\n",
       " ('100', '99'),\n",
       " ('102', '105'),\n",
       " ('100', '101', '98'),\n",
       " ('101', '102', '98'),\n",
       " ('103', '97', '99'),\n",
       " ('102', '97', '98'),\n",
       " ('100', '101', '102'),\n",
       " ('102', '98', '99'),\n",
       " ('102', '105', '98'),\n",
       " ('101', '97', '99'),\n",
       " ('103', '105', '99'),\n",
       " ('102', '103', '98'),\n",
       " ('102', '103', '97'),\n",
       " ('100', '102', '98'),\n",
       " ('97', '98', '99'),\n",
       " ('102', '105', '99'),\n",
       " ('103', '105', '98'),\n",
       " ('103', '98', '99'),\n",
       " ('102', '103', '105'),\n",
       " ('100', '101', '99'),\n",
       " ('105', '98', '99'),\n",
       " ('102', '103', '99'),\n",
       " ('102', '103', '105', '98'),\n",
       " ('102', '103', '98', '99'),\n",
       " ('102', '105', '98', '99'),\n",
       " ('100', '101', '102', '98'),\n",
       " ('103', '105', '98', '99'),\n",
       " ('102', '103', '105', '99')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_task_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_itemsets = reduce_task_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_freq_itemsets(partitionData: map, candidate_itemsets: list) -> tuple:\n",
    "    '''Count candidate itemset.The output is a set of key-value pairs (C, v),\n",
    "    where C is one of the candidate sets and v is the support for that itemset among\n",
    "    the baskets that were input to this Map task\n",
    "    \n",
    "    Args:\n",
    "        partitionData (map)\n",
    "    Returns:```````````````\n",
    "        (tuple): (C,v)\n",
    "    '''\n",
    "    candidates_count = defaultdict(int)\n",
    "    partitionData = list(partitionData)\n",
    "    for candidate in candidate_itemsets:\n",
    "        for _, item_list in partitionData:\n",
    "            if set(candidate) <= set(item_list):\n",
    "                candidates_count[candidate] += 1 # increase 1 count\n",
    "        yield (candidate, candidates_count[candidate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_task_2 = basket_rdd\\\n",
    "    .mapPartitions(lambda partition: count_freq_itemsets(partition, candidate_itemsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Reduce tasks take the itemsets they are given as keys and sum the associated values.\n",
    "# If candidate_itemset count > threshold return`` \n",
    "reduce_task_2 = map_task_2.\\\n",
    "    reduceByKey(lambda x,y: x + y).filter(lambda x: x[1] >= support_threshold).\\\n",
    "        map(lambda x: tuple(sorted(x[0]))).sortBy(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemset = reduce_task_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output_pairs(list_of_items: list) -> str:\n",
    "    '''Format list of tuple itemsets and convert them to string. Creates new line for each length of pair.\n",
    "\n",
    "    Args:\n",
    "        list_of_items (list): list of items\n",
    "    Returns:\n",
    "        (str):\n",
    "    '''\n",
    "    output_string = ''\n",
    "    last_length_pair = len(list_of_items[0])\n",
    "    for pair in list_of_items:\n",
    "        tmp_string = \"', '\".join(list(pair))\n",
    "        tmp_string = f\"('{tmp_string}'),\"\n",
    "        if len(pair) != last_length_pair:\n",
    "            last_length_pair = len(pair)\n",
    "            output_string = output_string[:-1] + '\\n' + '\\n'\n",
    "        output_string += tmp_string\n",
    "    return output_string[:-1] # Remove last comma before \\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(file_path: str, header_candidate: str, list_of_candidates: list, header_frequent_items: str, list_of_freq_items: list) -> None:\n",
    "    '''Save data to file\n",
    "    Args:\n",
    "        file_path (str)\n",
    "        header_candidate (str)\n",
    "        list_of_candidates (list)\n",
    "        header_frequent_items (str)\n",
    "        list_of_freq_items (list)\n",
    "    '''\n",
    "    with open(file_path, 'w') as fp:\n",
    "        fp.write(f'{header_candidate}:\\n')\n",
    "        output_pairs = format_output_pairs(list_of_candidates)\n",
    "        fp.write(output_pairs)\n",
    "        fp.write(f'\\n\\n{header_frequent_items}:\\n')\n",
    "        output_pairs = format_output_pairs(list_of_freq_items)\n",
    "        fp.write(output_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file('text.txt', 'Candidates', candidate_itemsets, 'Frequent Itemsets', frequent_itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../resource/asnlib/publicdata/ta_feng_all_months_merged.csv\"\n",
    "# Read File, skipping header\n",
    "ta_feng_data = sc.textFile(filepath).zipWithIndex().filter(lambda x: x[1] > 0).map(lambda line: line[0].split(\",\"))\n",
    "\n",
    "# Select Columns: DATE (string), CUSTOMER_ID (int) and PRODUCT_ID (int)\n",
    "# [1:-1] -> Remove \"\" \n",
    "ta_feng_data = ta_feng_data.map(lambda x: (x[0][1:-1], int(x[1][1:-1]), int(x[5][1:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_feng_data_preprocessed = ta_feng_data.map(lambda x: (f\"{x[0]}-{x[1]}\", x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(rows: list, columns: list, output_path: str) -> None:\n",
    "    '''Write rows to CSV\n",
    "\n",
    "    Args:\n",
    "        rows (list)\n",
    "        columns (list)\n",
    "        output_path (str)\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    with open(output_path, 'w') as csvfile:\n",
    "        # creating a csv writer object \n",
    "        csvwriter = csv.writer(csvfile)\n",
    "\n",
    "        # writing the fields\n",
    "        csvwriter.writerow(columns)\n",
    "\n",
    "        # writing the data rows \n",
    "        csvwriter.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = ta_feng_data_preprocessed.collect()\n",
    "\n",
    "file_path_output = 'costumer_product.csv'\n",
    "fields = [\"DATE-CUSTOMER_ID\", \"PRODUCT_ID\"]\n",
    "write_csv(rows, fields, file_path_output)\n",
    "\n",
    "# with open(file_path_output, 'w') as csvfile:\n",
    "#     # creating a csv writer object \n",
    "#     csvwriter = csv.writer(csvfile)\n",
    "\n",
    "#     # writing the fields\n",
    "#     csvwriter.writerow(fields)\n",
    "\n",
    "#     # writing the data rows \n",
    "#     csvwriter.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://sparkbyexamples.com/pyspark/pyspark-mappartitions/\n",
    "- https://www.geeksforgeeks.org/writing-csv-files-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed29fcc266c631214eb0a32dcb461d51a7279cf73c87ecfbb65b364d13f4dbad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
